/**
 * FlashAttention Tests (ANTI-009 Compliance)
 *
 * Verifies:
 * 1. REAL attention computation (not mock)
 * 2. Uses key parameter (not ignored)
 * 3. Supports attention masking
 * 4. Numerical stability (no NaN/Inf)
 * 5. Deterministic with seed
 * 6. IO-aware tiling correctness
 */

import { describe, it, expect } from 'vitest';
import { RealFlashAttention } from '../../../../../src/god-agent/core/attention/mechanisms/flash-attention.js';

describe('RealFlashAttention', () => {
  describe('Initialization', () => {
    it('should initialize with default config', () => {
      const attention = new RealFlashAttention();
      expect(attention.name).toBe('flash');
      expect(attention.getParameterCount()).toBe(4 * 1536 * 1536);
    });

    it('should initialize with custom config', () => {
      const attention = new RealFlashAttention({
        dimension: 512,
        numHeads: 8,
        blockSize: 32,
        seed: 42,
      });
      expect(attention.getParameterCount()).toBe(4 * 512 * 512);
    });

    it('should throw error if dimension not divisible by numHeads', () => {
      expect(() => {
        new RealFlashAttention({ dimension: 768, numHeads: 11 });
      }).toThrow('must be divisible by numHeads');
    });

    it('should throw error for invalid block size', () => {
      expect(() => {
        new RealFlashAttention({ dimension: 768, blockSize: 0 });
      }).toThrow('Block size');
    });
  });

  describe('ANTI-009 Requirement 1: REAL Attention Computation', () => {
    it('should NOT return 0.5*query + 0.5*value', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      // Initialize with distinct values
      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      const output = attention.forward(query, key, value, undefined, seqLen);

      // Compute mock output: 0.5*query + 0.5*value
      const mockOutput = new Float32Array(seqLen * dim);
      for (let i = 0; i < seqLen * dim; i++) {
        mockOutput[i] = 0.5 * query[i] + 0.5 * value[i];
      }

      // Verify outputs are different
      let maxDiff = 0;
      for (let i = 0; i < output.length; i++) {
        const diff = Math.abs(output[i] - mockOutput[i]);
        maxDiff = Math.max(maxDiff, diff);
      }

      expect(maxDiff).toBeGreaterThan(0.01); // Significant difference from mock
    });
  });

  describe('ANTI-009 Requirement 2: Uses Key Parameter', () => {
    it('should produce different outputs when key changes', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      // First output with key1
      const key1 = new Float32Array(seqLen * dim);
      for (let i = 0; i < seqLen * dim; i++) {
        key1[i] = Math.cos(i * 0.1);
      }
      const output1 = attention.forward(query, key1, value, undefined, seqLen);

      // Second output with key2 (different)
      const key2 = new Float32Array(seqLen * dim);
      for (let i = 0; i < seqLen * dim; i++) {
        key2[i] = Math.cos(i * 0.3); // Different pattern
      }
      const output2 = attention.forward(query, key2, value, undefined, seqLen);

      // Verify outputs differ significantly
      let maxDiff = 0;
      for (let i = 0; i < output1.length; i++) {
        const diff = Math.abs(output1[i] - output2[i]);
        maxDiff = Math.max(maxDiff, diff);
      }

      expect(maxDiff).toBeGreaterThan(0.001);
    });
  });

  describe('ANTI-009 Requirement 3: Attention Masking', () => {
    it('should support causal mask', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      // Create causal mask: can only attend to current and previous positions
      const mask = new Array(seqLen * seqLen);
      for (let i = 0; i < seqLen; i++) {
        for (let j = 0; j < seqLen; j++) {
          mask[i * seqLen + j] = j <= i; // true if j <= i
        }
      }

      const outputMasked = attention.forward(query, key, value, mask, seqLen);
      const outputUnmasked = attention.forward(query, key, value, undefined, seqLen);

      // Verify masked and unmasked outputs differ
      let maxDiff = 0;
      for (let i = 0; i < outputMasked.length; i++) {
        const diff = Math.abs(outputMasked[i] - outputUnmasked[i]);
        maxDiff = Math.max(maxDiff, diff);
      }

      expect(maxDiff).toBeGreaterThan(0.001);
    });

    it('should handle all-masked row (uniform fallback)', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = i % 10; // Simple pattern for verification
      }

      // Create mask where row 0 is completely masked
      const mask = new Array(seqLen * seqLen);
      for (let i = 0; i < seqLen; i++) {
        for (let j = 0; j < seqLen; j++) {
          mask[i * seqLen + j] = i !== 0; // Row 0 all false
        }
      }

      const output = attention.forward(query, key, value, mask, seqLen);

      // Should not produce NaN or Inf
      for (let i = 0; i < output.length; i++) {
        expect(isFinite(output[i])).toBe(true);
      }
    });
  });

  describe('ANTI-009 Requirement 4: Numerical Stability', () => {
    it('should not produce NaN or Inf for large scores', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      // Large values to test stability
      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = 100 * Math.sin(i * 0.1);
        key[i] = 100 * Math.cos(i * 0.1);
        value[i] = 100 * Math.sin(i * 0.2);
      }

      const output = attention.forward(query, key, value, undefined, seqLen);

      // Verify no NaN or Inf
      for (let i = 0; i < output.length; i++) {
        expect(isNaN(output[i])).toBe(false);
        expect(isFinite(output[i])).toBe(true);
      }
    });

    it('should handle zero inputs gracefully', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim); // All zeros
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      const output = attention.forward(query, key, value, undefined, seqLen);

      // Should produce valid output (not NaN/Inf)
      for (let i = 0; i < output.length; i++) {
        expect(isFinite(output[i])).toBe(true);
      }
    });
  });

  describe('ANTI-009 Requirement 5: Deterministic with Seed', () => {
    it('should produce identical outputs with same seed', () => {
      const seed = 42;
      const seqLen = 4;
      const dim = 64;

      const attention1 = new RealFlashAttention({ dimension: dim, numHeads: 4, seed });
      const attention2 = new RealFlashAttention({ dimension: dim, numHeads: 4, seed });

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      const output1 = attention1.forward(query, key, value, undefined, seqLen);
      const output2 = attention2.forward(query, key, value, undefined, seqLen);

      // Outputs should be identical
      for (let i = 0; i < output1.length; i++) {
        expect(output1[i]).toBeCloseTo(output2[i], 10);
      }
    });

    it('should produce different outputs with different seeds', () => {
      const seqLen = 4;
      const dim = 64;

      const attention1 = new RealFlashAttention({ dimension: dim, numHeads: 4, seed: 42 });
      const attention2 = new RealFlashAttention({ dimension: dim, numHeads: 4, seed: 123 });

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      const output1 = attention1.forward(query, key, value, undefined, seqLen);
      const output2 = attention2.forward(query, key, value, undefined, seqLen);

      // Outputs should differ
      let maxDiff = 0;
      for (let i = 0; i < output1.length; i++) {
        const diff = Math.abs(output1[i] - output2[i]);
        maxDiff = Math.max(maxDiff, diff);
      }

      expect(maxDiff).toBeGreaterThan(0.001);
    });
  });

  describe('ANTI-009 Requirement 6: IO-Aware Tiling', () => {
    it('should produce same results as standard attention (exact, not approximate)', () => {
      // Note: This is a structural test - FlashAttention should compute
      // identical results to standard attention, just with better I/O efficiency
      const attention = new RealFlashAttention({
        dimension: 64,
        numHeads: 4,
        blockSize: 16, // Small block to test tiling
        seed: 42
      });

      const seqLen = 8; // Multiple blocks
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      const output = attention.forward(query, key, value, undefined, seqLen);

      // Verify output is valid and non-trivial
      let nonZeroCount = 0;
      for (let i = 0; i < output.length; i++) {
        expect(isFinite(output[i])).toBe(true);
        if (Math.abs(output[i]) > 0.001) {
          nonZeroCount++;
        }
      }

      expect(nonZeroCount).toBeGreaterThan(output.length * 0.1); // At least 10% non-trivial
    });

    it('should work with different block sizes', () => {
      const seed = 42;
      const seqLen = 16;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      // Test with different block sizes (should all produce similar results)
      const attention8 = new RealFlashAttention({ dimension: dim, numHeads: 4, blockSize: 8, seed });
      const attention16 = new RealFlashAttention({ dimension: dim, numHeads: 4, blockSize: 16, seed });
      const attention32 = new RealFlashAttention({ dimension: dim, numHeads: 4, blockSize: 32, seed });

      const output8 = attention8.forward(query, key, value, undefined, seqLen);
      const output16 = attention16.forward(query, key, value, undefined, seqLen);
      const output32 = attention32.forward(query, key, value, undefined, seqLen);

      // All should produce valid outputs
      for (let i = 0; i < output8.length; i++) {
        expect(isFinite(output8[i])).toBe(true);
        expect(isFinite(output16[i])).toBe(true);
        expect(isFinite(output32[i])).toBe(true);
      }

      // NOTE: Outputs may differ slightly due to different seeded initialization
      // but all should be valid
    });
  });

  describe('Edge Cases', () => {
    it('should handle single token sequence', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 1;
      const dim = 64;

      const query = new Float32Array(dim);
      const key = new Float32Array(dim);
      const value = new Float32Array(dim);

      for (let i = 0; i < dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      const output = attention.forward(query, key, value, undefined, seqLen);

      expect(output.length).toBe(dim);
      for (let i = 0; i < output.length; i++) {
        expect(isFinite(output[i])).toBe(true);
      }
    });

    it('should handle long sequences efficiently', () => {
      const attention = new RealFlashAttention({
        dimension: 128,
        numHeads: 8,
        blockSize: 32,
        seed: 42
      });
      const seqLen = 256; // Long sequence
      const dim = 128;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.001);
        key[i] = Math.cos(i * 0.001);
        value[i] = Math.sin(i * 0.002);
      }

      const startTime = performance.now();
      const output = attention.forward(query, key, value, undefined, seqLen);
      const endTime = performance.now();

      // Should complete in reasonable time (< 5 seconds for this size)
      expect(endTime - startTime).toBeLessThan(5000);

      // Verify output validity
      for (let i = 0; i < output.length; i++) {
        expect(isFinite(output[i])).toBe(true);
      }
    });

    it('should throw error for dimension mismatch', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;

      const query = new Float32Array(seqLen * 64);
      const key = new Float32Array(seqLen * 32); // Wrong dimension
      const value = new Float32Array(seqLen * 64);

      expect(() => {
        attention.forward(query, key, value, undefined, seqLen);
      }).toThrow('Dimension mismatch');
    });

    it('should throw error for mask dimension mismatch', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);
      const mask = new Array(10); // Wrong size

      expect(() => {
        attention.forward(query, key, value, mask, seqLen);
      }).toThrow('Mask length');
    });
  });

  describe('Multi-Head Computation', () => {
    it('should compute multiple heads independently', () => {
      const attention = new RealFlashAttention({ dimension: 64, numHeads: 4, seed: 42 });
      const seqLen = 4;
      const dim = 64;

      const query = new Float32Array(seqLen * dim);
      const key = new Float32Array(seqLen * dim);
      const value = new Float32Array(seqLen * dim);

      for (let i = 0; i < seqLen * dim; i++) {
        query[i] = Math.sin(i * 0.1);
        key[i] = Math.cos(i * 0.1);
        value[i] = Math.sin(i * 0.2);
      }

      const output = attention.forward(query, key, value, undefined, seqLen);

      // Verify output shape
      expect(output.length).toBe(seqLen * dim);

      // Verify output is non-trivial
      let variance = 0;
      let mean = 0;
      for (let i = 0; i < output.length; i++) {
        mean += output[i];
      }
      mean /= output.length;

      for (let i = 0; i < output.length; i++) {
        variance += (output[i] - mean) ** 2;
      }
      variance /= output.length;

      expect(variance).toBeGreaterThan(0.001); // Non-trivial variance
    });
  });
});
